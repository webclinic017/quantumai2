{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import joblib\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from env.flo_portfolio import FlorianPortfolioEnv\n",
    "from env.portfolio_multi_reward import FlorianPortfolioEnvMultiReward\n",
    "from env.portfolio_bbg import Portfolio_BBG\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback,CallbackList, EvalCallback\n",
    "from models.callbacks import CustomCallBack, HParamCallback, TensorboardCallback\n",
    "from models.models import DRLAgent\n",
    "from gymnasium import spaces\n",
    "from pyfolio import timeseries\n",
    "import pyfolio\n",
    "from plot.plot import convert_daily_return_to_pyfolio_ts, convert_account_value_to_pyfolio_ts, get_baseline, backtest_stats, get_daily_return\n",
    "from hyperoptimizer.optunaoptimizer import optimize_optuna, optimize_optuna_FlorianPortfolioEnvMultiReward, optimize_optuna_BBG_Env\n",
    "from config import tickers\n",
    "from preprocessors.preprocessors import DataProcessor\n",
    "from wandb_env import wandb_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SPX_2.csv', skiprows=6).dropna(axis=0)\n",
    "df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = \"1990-01-03\"\n",
    "train_end = \"2015-12-31\"\n",
    "validate_start = \"2016-01-01\"\n",
    "validate_end = \"2020-12-31\"\n",
    "test_start = \"2016-01-01\"\n",
    "test_end = \"2023-07-25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, start, end, target_date_col=\"date\"):\n",
    "    data = df[(df[target_date_col] >= start) & (df[target_date_col] < end)]\n",
    "    data = data.sort_values([target_date_col, \"tic\"], ignore_index=True)\n",
    "    data.index = data[target_date_col].factorize()[0]\n",
    "    return data\n",
    "\n",
    "train = data_split(df, train_start, train_end)\n",
    "validate = data_split(df, validate_start, validate_end)\n",
    "train_df = train\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list = train.tic.unique()\n",
    "stock_list = stock_list.tolist()\n",
    "indicators = ['RSI14', 'RSI30', 'RSI3','MA200', 'MA50', 'MA20']\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1+2*stock_dimension + len(indicators)\n",
    "additional_price_info= ['open', 'low', 'high']\n",
    "env_kwargs = {\n",
    "    \"initial_amount\": 1000000, \n",
    "    \"trade_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"stock_list\": stock_list,\n",
    "    \"indicators\": indicators, \n",
    "    \"action_space\": stock_dimension,\n",
    "    \"sharpe_ratio_weight\": 0.1, #! to fine tune\n",
    "    \"loss_penalty_weight\": 0.1, #! to fine tune\n",
    "    \"short_selling_allowed\": True,\n",
    "    \"take_leverage_allowed\": True,\n",
    "    \"reward_scaling\": 0.00022786244568524788, # the magnitude of rewards can significantly affect the learning process. If the rewards are too large, they can cause the learning algorithm to become unstable. On the other hand, if the rewards are too small, the agent might not learn effectively because the rewards don't provide a strong enough signal.\n",
    "    \"hmax\": 100, #! Fine tune\n",
    "    # Not for optimization\n",
    "    \"make_plots\": False,\n",
    "    \"num_stock_shares\": [0], #number of initioal shares\n",
    "    \"model_name\": \"A-2C\",\n",
    "    \"mode\": \"training\", #can be anything, just for plots\n",
    "    \"iteration\": \"1000\"#can be anything, just for plots\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_opt(environment=FlorianPortfolioEnvMultiReward, train_df=train_df, project_name=\"portfolio1\", state_space=state_space, stock_list=stock_list, indicators=indicators, stock_dimension=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
